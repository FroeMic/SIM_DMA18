{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining & Analytics\n",
    "## Lab 6 (B)\n",
    "\n",
    "Available software:\n",
    " - Python's Gensim module: https://radimrehurek.com/gensim/ (install using pip)\n",
    " - Sklearn’s  TSNE module in case you use TSNE to reduce dimension (optional)\n",
    " - Python’s Matplotlib (optional)\n",
    "\n",
    "_Note: The most important hyper parameters of skip-gram/CBOW are vector size and windows size_\n",
    "\n",
    "This assignment  will be broadly  split into **2 parts**.\n",
    "\n",
    "### Lab06 (A)\n",
    "\n",
    "#### Part I\n",
    "##### Preparation:\n",
    "Download and extract the Google’s pretrained Word2Vec model (Google has  trained a large corpus of text containing billions of words,). To kick things off we will use this pre trained model to explore Word2Vec. \n",
    "(Download Link: https://docs.google.com/a/berkeley.edu/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download)\n",
    "Now load this pretrained model in Gensim and you should be good to get started with this assignment. \n",
    "\n",
    "[ ... ] (Omitted)\n",
    "\n",
    "#### Part II:\n",
    "\n",
    "In part 1 we used the Word2Vec model on a pre trained corpus. In this part (in the next lab) you are going to train a Word2Vec model on your own dataset/corpus(text). Choose a text corpus (A good place to start will be the nltk corpus, the gutenburg project or the brown movie reviews) and tokenize the text (We will go through this in detail in the next Lab.) \n",
    "\n",
    "You can also choose the the dataset provided here.\n",
    "\n",
    "Q7. Based on your knowledge or understand of the text corpus you have chosen, form 3 hypotheses of analogies or relationships you expect will hold and give a reason why.\n",
    "\n",
    "\n",
    "### Lab06 (B)\n",
    "\n",
    " \n",
    "1. Generate embeddings from the corpus you had chosen in the previous lab.\n",
    "2. Verify and test your hypotheses from the previous lab.\n",
    "3. Use T-SNE or PCA to reduce the dimensionality of the vectors to two dimensions for: \n",
    "    1. The GoogleNews corpus. Feel free to down sample it to 10 - 20k words based on frequency.\n",
    "    2. The embeddings you just generated.\n",
    "4. Using [this](https://github.com/CAHLR/d3-scatterplot) library, visualize both reduced datasets from the step above and explore the visualization.\n",
    "5. Submit the jupyter notebook (including lab6a) with the code and an embedded (screengrabbed) image of the viz(s) that you created and explored. Ensure that you have appropriate comments for both the code and the images.\n",
    "6. For this library, you need to generate a tab separated text file in the following format: `Dim1 | Dim2 | Label (word)`\n",
    "\n",
    "#### How to use the JS library\n",
    "\n",
    "```\n",
    "1) copy over the d3-scatterplot .html and .js files onto your machine\n",
    "2) setup a local web server in the directory of the d3-scatterplot files (on terminal - \"python -m http.server\" for Python 3.x)\n",
    "3) place a tab separated file in that same folder \n",
    "4) the tab separated file must have at least an x and y column and a third column of any value (in our case, the word itself)\n",
    "5) go to http://localhost:8000/plot.html?dataset=name_of_dataset.txt (Links to an external site.)Links to an external site. (Links to an external site.)Links to an external site.\n",
    "6) Hover over plot points to see description in the upper left. You can color the points (using cluster labels, perhaps) using additional columns in your text file.\n",
    "7) lasso select a group of points with the left mouse button and look at summaries of the group to the right and all the selected point descriptions below the plot\n",
    " \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I:\n",
    "\n",
    "\n",
    "# [ ... ]\n",
    "\n",
    "(**Note:** *Omitted code from Lab06a Part I. Have a look at the seperately submitted jupyter notebook for Lab06A*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part II:\n",
    "In the next lab you are going to train a Word2Vec model on your own dataset/corpus(text). To prepare do the following...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a text corpus (A good place to start will be the nltk corpus, the gutenburg project or the brown movie reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** I decided to select [Grimms' Fairy Tales](https://www.gutenberg.org/ebooks/2591) from Project Gutenberg for the next lab and analyse it using the NLTK corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text (We will go through this in detail in the next Lab.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import string\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e2815454da7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Note: Code adapted from gensim_tutorial.ipynb from the current lab.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msent_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m### Download and load  \"The Importance of Being Earnest A Trivial Comedy for Serious People\" by Oscar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# Note: Code adapted from gensim_tutorial.ipynb from the current lab.\n",
    "\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "### Download and load  \"The Importance of Being Earnest A Trivial Comedy for Serious People\" by Oscar \n",
    "## Wilde from Project Gutenberg : https://www.gutenberg.org\n",
    "\n",
    "\n",
    "## URL of Grimms's Fairy Tales\n",
    "url = \"https://www.gutenberg.org/files/2591/2591-0.txt\" ## Your raw text file location \n",
    "resp = urlopen(url)\n",
    "raw = resp.read().decode('utf8')\n",
    "firstlook = tokenize.sent_tokenize(raw)\n",
    "\n",
    "pattern = r'''(?x)  # set flag to allow verbose regexps\n",
    "(?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "|\\w+(?:[-']\\w+)*    # words with optional internal hyphens\n",
    "|\\$?\\d+(?:\\.\\d+)?   # currency, e.g. $12.80 \n",
    "|\\.\\.\\.             # elipses\n",
    "|[.,;\"'?()-_`]      # these are separate tokens\n",
    "'''\n",
    "#print(nltk.regexp_tokenize(raw,pattern))\n",
    "tokenized_raw = ' '.join( nltk.regexp_tokenize(raw,pattern))\n",
    "tokenized_raw= tokenize.sent_tokenize(tokenized_raw)\n",
    "\n",
    "nopunct=[]\n",
    "for sent in tokenized_raw:\n",
    "    a=[w for w in sent.split() if w not in string.punctuation]\n",
    "    nopunct.append(' '.join(a))\n",
    "#create a set of stopwords\n",
    "tok_corp= [nltk.word_tokenize(sent) for sent in nopunct]\n",
    "\n",
    "### creating a list of unique words \n",
    "\n",
    "combined_list=[\" \".join(w) for w in tok_corp]\n",
    "unique_list=[]\n",
    "for sent in combined_list:\n",
    "    unique_list.append([w for w in sent.split()])\n",
    "unique_list=list(set([item for sublist in unique_list for item in sublist]))\n",
    "\n",
    "unique_words=unique_list\n",
    "\n",
    "### Its just one single command\n",
    "model = gensim.models.Word2Vec(tok_corp, min_count=1, size = 16, window=7)\n",
    "\n",
    "## Extracting the respective vectors corresponding to the words\n",
    "\n",
    "vector_list=[] ## n by d matrix containing words and their respective vectors\n",
    "for word in unique_words:\n",
    "    vector_list.append(model[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Based on your knowledge or understand of the text corpus you have chosen, form 3 hypotheses of analogies or relationships you expect will hold and give a reason why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** *I created the hyptotheses below based on my knowledge of the stories within the Brother Grimm's Fary Tales. I did not inspect the text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H1: The words 'wolf' and 'evil' will have a high similarity.\n",
    "\n",
    "**Reason:** The character of the wolf is commonly associated as the evil persona in fary tales. Examples are *Red Riding Hood*, *Three Little Pigs* and probably there are a whole bunch more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2: 'King' + 'Daughter' = 'Princess'\n",
    "\n",
    "**Reason:** My assumption here is that the above relationship is present in enough fary tales that it can be 'uncovered' by building a corpus out of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H3: 'Hansel' + 'sister' = 'Gretel'\n",
    "\n",
    "**Reason:** The last hypotheses is specific to one of the many fary tales — *Hansel and Gretel*. The are siblings, so I assume that the above relationship can be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
