{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining & Analytics\n",
    "## Lab 6 (B)\n",
    "\n",
    "Available software:\n",
    " - Python's Gensim module: https://radimrehurek.com/gensim/ (install using pip)\n",
    " - Sklearn’s  TSNE module in case you use TSNE to reduce dimension (optional)\n",
    " - Python’s Matplotlib (optional)\n",
    "\n",
    "_Note: The most important hyper parameters of skip-gram/CBOW are vector size and windows size_\n",
    "\n",
    "This assignment  will be broadly  split into **2 parts**.\n",
    "\n",
    "### Lab06 (A)\n",
    "\n",
    "#### Part I\n",
    "##### Preparation:\n",
    "Download and extract the Google’s pretrained Word2Vec model (Google has  trained a large corpus of text containing billions of words,). To kick things off we will use this pre trained model to explore Word2Vec. \n",
    "(Download Link: https://docs.google.com/a/berkeley.edu/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download)\n",
    "Now load this pretrained model in Gensim and you should be good to get started with this assignment. \n",
    "\n",
    "[ ... ] (Omitted)\n",
    "\n",
    "#### Part II:\n",
    "\n",
    "In part 1 we used the Word2Vec model on a pre trained corpus. In this part (in the next lab) you are going to train a Word2Vec model on your own dataset/corpus(text). Choose a text corpus (A good place to start will be the nltk corpus, the gutenburg project or the brown movie reviews) and tokenize the text (We will go through this in detail in the next Lab.) \n",
    "\n",
    "You can also choose the the dataset provided here.\n",
    "\n",
    "Q7. Based on your knowledge or understand of the text corpus you have chosen, form 3 hypotheses of analogies or relationships you expect will hold and give a reason why.\n",
    "\n",
    "\n",
    "### Lab06 (B)\n",
    "\n",
    " \n",
    "1. Generate embeddings from the corpus you had chosen in the previous lab.\n",
    "2. Verify and test your hypotheses from the previous lab.\n",
    "3. Use T-SNE or PCA to reduce the dimensionality of the vectors to two dimensions for: \n",
    "    1. The GoogleNews corpus. Feel free to down sample it to 10 - 20k words based on frequency.\n",
    "    2. The embeddings you just generated.\n",
    "4. Using [this](https://github.com/CAHLR/d3-scatterplot) library, visualize both reduced datasets from the step above and explore the visualization.\n",
    "5. Submit the jupyter notebook (including lab6a) with the code and an embedded (screengrabbed) image of the viz(s) that you created and explored. Ensure that you have appropriate comments for both the code and the images.\n",
    "6. For this library, you need to generate a tab separated text file in the following format: `Dim1 | Dim2 | Label (word)`\n",
    "\n",
    "#### How to use the JS library\n",
    "\n",
    "```\n",
    "1) copy over the d3-scatterplot .html and .js files onto your machine\n",
    "2) setup a local web server in the directory of the d3-scatterplot files (on terminal - \"python -m http.server\" for Python 3.x)\n",
    "3) place a tab separated file in that same folder \n",
    "4) the tab separated file must have at least an x and y column and a third column of any value (in our case, the word itself)\n",
    "5) go to http://localhost:8000/plot.html?dataset=name_of_dataset.txt (Links to an external site.)Links to an external site. (Links to an external site.)Links to an external site.\n",
    "6) Hover over plot points to see description in the upper left. You can color the points (using cluster labels, perhaps) using additional columns in your text file.\n",
    "7) lasso select a group of points with the left mouse button and look at summaries of the group to the right and all the selected point descriptions below the plot\n",
    " \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import string\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity_for_2_words(model, left, right):\n",
    "    M = np.array([model[left], model[right]])\n",
    "    M_sim = cosine_similarity(M)\n",
    "    return M_sim[0][1]\n",
    "\n",
    "def euc_similarity_for_2_words(model, left, right):\n",
    "    return np.linalg.norm(model[left]-model[right])\n",
    "\n",
    "def print_cos_similarity_for_word_pair(model, pair):\n",
    "    print(\"{}: {:.4f} \".format(pair, cos_similarity_for_2_words(model, pair[0], pair[1])))\n",
    "\n",
    "def print_euc_similarity_for_word_pair(model, pair):\n",
    "    print(\"{}: {:.4f} \".format(pair, euc_similarity_for_2_words(model, pair[0], pair[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I:\n",
    "\n",
    "\n",
    "# [ ... ]\n",
    "\n",
    "(**Note:** *Omitted code from Lab06a Part I. Have a look at the seperately submitted jupyter notebook for Lab06A*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part II:\n",
    "In the next lab you are going to train a Word2Vec model on your own dataset/corpus(text). To prepare do the following...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a text corpus (A good place to start will be the nltk corpus, the gutenburg project or the brown movie reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** I decided to select [Grimms' Fairy Tales](https://www.gutenberg.org/ebooks/2591) from Project Gutenberg for the next lab and analyse it using the NLTK corpus and selected the following three hypotheses.\n",
    "\n",
    "\n",
    "##### H1: The words 'wolf' and 'evil' will have a high similarity.\n",
    "**Reason:** The character of the wolf is commonly associated as the evil persona in fary tales. Examples are Red Riding Hood, Three Little Pigs and probably there are a whole bunch more.\n",
    "\n",
    "##### H2: 'King' + 'Daughter' = 'Princess'\n",
    "**Reason:** My assumption here is that the above relationship is present in enough fary tales that it can be 'uncovered' by building a corpus out of this book.\n",
    "\n",
    "##### H3: 'Hansel' + 'sister' = 'Gretel'\n",
    "**Reason:** The last hypotheses is specific to one of the many fary tales — Hansel and Gretel. The are siblings, so I assume that the above relationship can be detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab06 B\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text (We will go through this in detail in the next Lab.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/Desktop/Repositories/SIM/DataMining/.dma/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# Note: Code adapted from gensim_tutorial.ipynb from the current lab.\n",
    "\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "### Download and load  \"The Importance of Being Earnest A Trivial Comedy for Serious People\" by Oscar \n",
    "## Wilde from Project Gutenberg : https://www.gutenberg.org\n",
    "\n",
    "\n",
    "## URL of Grimms's Fairy Tales\n",
    "url = \"https://www.gutenberg.org/files/2591/2591-0.txt\" ## Your raw text file location \n",
    "resp = urlopen(url)\n",
    "raw = resp.read().decode('utf8')\n",
    "firstlook = tokenize.sent_tokenize(raw)\n",
    "\n",
    "pattern = r'''(?x)  # set flag to allow verbose regexps\n",
    "(?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "|\\w+(?:[-']\\w+)*    # words with optional internal hyphens\n",
    "|\\$?\\d+(?:\\.\\d+)?   # currency, e.g. $12.80 \n",
    "|\\.\\.\\.             # elipses\n",
    "|[.,;\"'?()-_`]      # these are separate tokens\n",
    "'''\n",
    "#print(nltk.regexp_tokenize(raw,pattern))\n",
    "tokenized_raw = ' '.join( nltk.regexp_tokenize(raw,pattern))\n",
    "tokenized_raw= tokenize.sent_tokenize(tokenized_raw)\n",
    "\n",
    "nopunct=[]\n",
    "for sent in tokenized_raw:\n",
    "    a=[w for w in sent.split() if w not in string.punctuation]\n",
    "    nopunct.append(' '.join(a))\n",
    "#create a set of stopwords\n",
    "tok_corp= [nltk.word_tokenize(sent) for sent in nopunct]\n",
    "\n",
    "### creating a list of unique words \n",
    "\n",
    "combined_list=[\" \".join(w) for w in tok_corp]\n",
    "unique_list=[]\n",
    "for sent in combined_list:\n",
    "    unique_list.append([w for w in sent.split()])\n",
    "unique_list=list(set([item for sublist in unique_list for item in sublist]))\n",
    "\n",
    "unique_words=unique_list\n",
    "\n",
    "### Its just one single command\n",
    "model = gensim.models.Word2Vec(tok_corp, min_count=1, size = 16, window=7)\n",
    "\n",
    "## Extracting the respective vectors corresponding to the words\n",
    "\n",
    "vector_list=[] ## n by d matrix containing words and their respective vectors\n",
    "for word in unique_words:\n",
    "    vector_list.append(model[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.05235949, -0.04953231,  0.03576385, -0.01601919, -0.0693511 ,\n",
      "       -0.01456504,  0.01359499, -0.01525817,  0.085245  ,  0.00977326,\n",
      "       -0.0210762 ,  0.00151788,  0.04061151, -0.01398589,  0.01363964,\n",
      "        0.04431199], dtype=float32), array([ 0.02624226, -0.01981116,  0.03476242, -0.00232454, -0.04765066,\n",
      "       -0.0327542 , -0.00440537,  0.0152738 ,  0.07384662,  0.00578531,\n",
      "       -0.00191476,  0.00331511,  0.03962994, -0.00388025, -0.01304295,\n",
      "        0.01633914], dtype=float32), array([ 0.09900264, -0.0471682 ,  0.07363572, -0.04363722, -0.07313373,\n",
      "       -0.04096917, -0.07151716, -0.00578928,  0.17756607,  0.00371851,\n",
      "        0.00570008, -0.01576044,  0.13996762, -0.02839047, -0.01232996,\n",
      "        0.04880045], dtype=float32), array([ 0.18350405, -0.09647742,  0.14782602, -0.0462269 , -0.13688937,\n",
      "       -0.08243618, -0.03592265,  0.02405367,  0.27839208, -0.04941341,\n",
      "       -0.01147962, -0.03909972,  0.21690543, -0.01661538,  0.02661334,\n",
      "        0.08772004], dtype=float32), array([ 0.00332768,  0.01575499,  0.02376211,  0.0185231 , -0.03803491,\n",
      "       -0.0029692 , -0.00942367, -0.00949477,  0.01174269, -0.03668582,\n",
      "        0.00152433,  0.01305315,  0.02814895, -0.01717377,  0.03403957,\n",
      "        0.02845769], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(vector_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify and test your hypotheses from the previous lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### H1: The words 'wolf' and 'evil' will have a high similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('wolf', 'evil'): 0.9923 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/Desktop/Repositories/SIM/DataMining/.dma/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print_cos_similarity_for_word_pair(model, ('wolf', 'evil'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** A cosine similiarity of 0.9923 speak for a high correlation between the words. Hypothesis confirmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### H2: 'King' + 'Daughter' = 'Princess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chamber', 0.9985895156860352), ('fox', 0.9984092712402344), ('finger', 0.998346745967865), ('put', 0.9980343580245972), ('heard', 0.9980288743972778), ('Then', 0.9979771971702576), ('morning', 0.9979766607284546), ('in', 0.9979627728462219), ('room', 0.997961699962616), ('ring', 0.9979234933853149)]\n",
      "\n",
      "[Rank 195] :: ('princess', 0.9941306710243225)\n",
      "\n",
      "('king', 'princess'): 0.9867 \n",
      "('daughter', 'princess'): 0.9980 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/Desktop/Repositories/SIM/DataMining/.dma/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/mike/Desktop/Repositories/SIM/DataMining/.dma/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "top_1000 = model.most_similar(positive=['king','daughter'], topn=1000)\n",
    "print(top_1000[:10])\n",
    "print()\n",
    "\n",
    "for i, (word, sim) in enumerate(top_1000):\n",
    "    if word == 'princess':\n",
    "        print('[Rank ' + str(i) + ']','::', (word, sim))\n",
    "\n",
    "print()\n",
    "print_cos_similarity_for_word_pair(model, ('king', 'princess'))\n",
    "print_cos_similarity_for_word_pair(model, ('daughter', 'princess'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** Both ('king', 'princess') and ('daughter', 'princess') have a high similiarity. \n",
    "    However the combination of 'King' + 'Daughter' = 'Princess' is not confirmed, as there are more similiar results.\n",
    "    Inspecting the results further shows that 'King' + 'Daughter' = 'Princess' is only ranked 195."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### H3: 'Hansel' + 'sister' = 'Gretel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('an', 0.99968421459198), ('fine', 0.9995608329772949), ('however', 0.9995306730270386), ('people', 0.9995037317276001), ('to', 0.9994958639144897), ('without', 0.9994885921478271), ('four', 0.999474823474884), ('heart', 0.9994156360626221), ('large', 0.9994149804115295), ('mother', 0.9993584156036377)]\n",
      "\n",
      "[Rank 226] :: ('Gretel', 0.9975356459617615)\n",
      "\n",
      "('Hansel', 'Gretel'): 0.9979 \n",
      "('sister', 'Gretel'): 0.9967 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/Desktop/Repositories/SIM/DataMining/.dma/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/mike/Desktop/Repositories/SIM/DataMining/.dma/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "top_1000 = model.most_similar(positive=['Hansel','sister'], topn=1000)\n",
    "print(top_1000[:10])\n",
    "print()\n",
    "\n",
    "for i, (word, sim) in enumerate(top_1000):\n",
    "    if word == 'Gretel':\n",
    "        print('[Rank ' + str(i) + ']','::', (word, sim))\n",
    "\n",
    "print()\n",
    "print_cos_similarity_for_word_pair(model, ('Hansel', 'Gretel'))\n",
    "print_cos_similarity_for_word_pair(model, ('sister', 'Gretel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** As with the previous hypothesis, also here the assumption does not hold. 'Hansel' + 'sister' = 'Gretel' is only ranked 226. Even though their is a high similiaity, the formula 'Hansel' + 'sister' does not seem to be descriptive enough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use T-SNE or PCA to reduce the dimensionality of the vectors to two dimensions for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The GoogleNews corpus. Feel free to down sample it to 10 - 20k words based on frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_news_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample words by frequency\n",
    "n = 20000\n",
    "\n",
    "words = list(google_news_model.vocab)\n",
    "word_count_list = []\n",
    "\n",
    "for word in words:\n",
    "    word_count_list.append((word, google_news_model.vocab[word].count))\n",
    "    \n",
    "sorted_wort_count_list = sorted(word_count_list, reverse=True, key=lambda word_count: word_count[1]) \n",
    "\n",
    "sampled_words = [word_count[0] for word_count in sorted_wort_count_list[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_news_vector_list=[] ## n by d matrix containing words and their respective vectors\n",
    "for word in sampled_words:\n",
    "    google_news_vector_list.append(google_news_model[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 151 nearest neighbors...\n",
      "[t-SNE] Indexed 20000 samples in 0.281s...\n",
      "[t-SNE] Computed neighbors for 20000 samples in 367.975s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 20000\n",
      "[t-SNE] Mean sigma: 0.993958\n",
      "[t-SNE] Computed conditional probabilities in 1.393s\n",
      "[t-SNE] Iteration 50: error = 102.8054504, gradient norm = 0.0887065 (50 iterations in 78.852s)\n",
      "[t-SNE] Iteration 100: error = 103.3485565, gradient norm = 0.0861337 (50 iterations in 77.758s)\n",
      "[t-SNE] Iteration 150: error = 104.7123260, gradient norm = 0.0807695 (50 iterations in 71.091s)\n",
      "[t-SNE] Iteration 200: error = 104.1133728, gradient norm = 0.0690418 (50 iterations in 68.669s)\n",
      "[t-SNE] Iteration 250: error = 103.4638062, gradient norm = 0.0950888 (50 iterations in 62.997s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 103.463806\n",
      "[t-SNE] Iteration 300: error = 4.4973211, gradient norm = 0.0038015 (50 iterations in 62.758s)\n",
      "[t-SNE] Iteration 350: error = 3.9228616, gradient norm = 0.0025561 (50 iterations in 54.468s)\n",
      "[t-SNE] Iteration 400: error = 3.6397510, gradient norm = 0.0006140 (50 iterations in 52.760s)\n",
      "[t-SNE] Iteration 450: error = 3.4989755, gradient norm = 0.0002615 (50 iterations in 47.710s)\n",
      "[t-SNE] Iteration 500: error = 3.3969486, gradient norm = 0.0002006 (50 iterations in 46.025s)\n",
      "[t-SNE] Iteration 550: error = 3.3193762, gradient norm = 0.0001591 (50 iterations in 46.202s)\n",
      "[t-SNE] Iteration 600: error = 3.2550750, gradient norm = 0.0001306 (50 iterations in 58.444s)\n",
      "[t-SNE] Iteration 650: error = 3.2024822, gradient norm = 0.0001103 (50 iterations in 57.318s)\n",
      "[t-SNE] Iteration 700: error = 3.1585732, gradient norm = 0.0000958 (50 iterations in 54.958s)\n",
      "[t-SNE] Iteration 750: error = 3.1212251, gradient norm = 0.0000845 (50 iterations in 44.518s)\n",
      "[t-SNE] Iteration 800: error = 3.0895226, gradient norm = 0.0000748 (50 iterations in 44.703s)\n",
      "[t-SNE] Iteration 850: error = 3.0619271, gradient norm = 0.0000670 (50 iterations in 45.075s)\n",
      "[t-SNE] Iteration 900: error = 3.0375912, gradient norm = 0.0000610 (50 iterations in 46.635s)\n",
      "[t-SNE] Iteration 950: error = 3.0162187, gradient norm = 0.0000556 (50 iterations in 47.224s)\n",
      "[t-SNE] Iteration 1000: error = 2.9974117, gradient norm = 0.0000514 (50 iterations in 45.247s)\n",
      "[t-SNE] Error after 1000 iterations: 2.997412\n"
     ]
    }
   ],
   "source": [
    "# Lets dim reduce the 16 dimension vectors to 2dimensions to vizualise the dataset \n",
    "data_embed=TSNE(n_components=2, perplexity=50, verbose=2, method='barnes_hut').fit_transform(google_news_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as tsv in format `Dim1 | Dim2 | Label (word)`\n",
    "df = pd.DataFrame(data_embed)\n",
    "df['word'] = sampled_words\n",
    "df.columns = ['x', 'y', 'label']\n",
    "df.head()\n",
    "\n",
    "df.to_csv('d3-scatterplot/google_news.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The embeddings you just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/Desktop/Repositories/SIM/DataMining/.dma/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "vector_list=[] ## n by d matrix containing words and their respective vectors\n",
    "for word in unique_words:\n",
    "    vector_list.append(model[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 151 nearest neighbors...\n",
      "[t-SNE] Indexed 5931 samples in 0.007s...\n",
      "[t-SNE] Computed neighbors for 5931 samples in 0.919s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 5931\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 5931\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 5931\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 5931\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 5931\n",
      "[t-SNE] Computed conditional probabilities for sample 5931 / 5931\n",
      "[t-SNE] Mean sigma: 0.031960\n",
      "[t-SNE] Computed conditional probabilities in 0.440s\n",
      "[t-SNE] Iteration 50: error = 81.7736130, gradient norm = 0.0409066 (50 iterations in 12.875s)\n",
      "[t-SNE] Iteration 100: error = 74.6888275, gradient norm = 0.0062669 (50 iterations in 8.772s)\n",
      "[t-SNE] Iteration 150: error = 74.1573410, gradient norm = 0.0022043 (50 iterations in 9.863s)\n",
      "[t-SNE] Iteration 200: error = 74.0356598, gradient norm = 0.0016372 (50 iterations in 15.362s)\n",
      "[t-SNE] Iteration 250: error = 73.9715729, gradient norm = 0.0013540 (50 iterations in 13.102s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 73.971573\n",
      "[t-SNE] Iteration 300: error = 2.4895730, gradient norm = 0.0010522 (50 iterations in 12.585s)\n",
      "[t-SNE] Iteration 350: error = 2.2500629, gradient norm = 0.0003985 (50 iterations in 16.541s)\n",
      "[t-SNE] Iteration 400: error = 2.1499510, gradient norm = 0.0002403 (50 iterations in 13.487s)\n",
      "[t-SNE] Iteration 450: error = 2.0930138, gradient norm = 0.0001585 (50 iterations in 13.059s)\n",
      "[t-SNE] Iteration 500: error = 2.0561008, gradient norm = 0.0001165 (50 iterations in 12.027s)\n",
      "[t-SNE] Iteration 550: error = 2.0309575, gradient norm = 0.0000920 (50 iterations in 13.673s)\n",
      "[t-SNE] Iteration 600: error = 2.0129411, gradient norm = 0.0000757 (50 iterations in 13.117s)\n",
      "[t-SNE] Iteration 650: error = 1.9997803, gradient norm = 0.0000658 (50 iterations in 13.618s)\n",
      "[t-SNE] Iteration 700: error = 1.9902718, gradient norm = 0.0000592 (50 iterations in 13.193s)\n",
      "[t-SNE] Iteration 750: error = 1.9830505, gradient norm = 0.0000544 (50 iterations in 13.474s)\n",
      "[t-SNE] Iteration 800: error = 1.9778793, gradient norm = 0.0000515 (50 iterations in 14.744s)\n",
      "[t-SNE] Iteration 850: error = 1.9739714, gradient norm = 0.0000533 (50 iterations in 12.383s)\n",
      "[t-SNE] Iteration 900: error = 1.9708140, gradient norm = 0.0000453 (50 iterations in 13.499s)\n",
      "[t-SNE] Iteration 950: error = 1.9682851, gradient norm = 0.0000428 (50 iterations in 14.921s)\n",
      "[t-SNE] Iteration 1000: error = 1.9658591, gradient norm = 0.0000399 (50 iterations in 13.219s)\n",
      "[t-SNE] Error after 1000 iterations: 1.965859\n"
     ]
    }
   ],
   "source": [
    "# Lets dim reduce the 16 dimension vectors to 2dimensions to vizualise the dataset \n",
    "data_embed=TSNE(n_components=2, perplexity=50, verbose=2, method='barnes_hut').fit_transform(vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as tsv in format `Dim1 | Dim2 | Label (word)`\n",
    "df = pd.DataFrame(data_embed)\n",
    "df['word'] = unique_words\n",
    "df.columns = ['x', 'y', 'label']\n",
    "df.head()\n",
    "\n",
    "df.to_csv('d3-scatterplot/hansel_gretel.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize both reduced datasets from the step above and explore the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The GoogleNews corpus. Feel free to down sample it to 10 - 20k words based on frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the full visualization of the 20k Google News Corpus Dataset. \n",
    "\n",
    "![Google News Corpus Full](./img/ss_viz_google_news_corpus_01.png)\n",
    "\n",
    "Zooming in on the cluster on the very top, we can (for example) find that the respective cluster contains capitals around the world.\n",
    "\n",
    "![Google News Corpus Full](./img/ss_viz_google_news_corpus_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The embeddings you just generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the full visualization of the Fary Tale Corpus. (It has quite a funny shape) \n",
    "\n",
    "![Hansel Gretel Corpus Full](./img/ss_viz_hansel_gretel_corpus_01.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
