{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining & Analytics\n",
    "## March 15 , Lab 6 (A): Skip Gram models\n",
    "\n",
    "Available software:\n",
    " - Python's Gensim module: https://radimrehurek.com/gensim/ (install using pip)\n",
    " - Sklearn’s  TSNE module in case you use TSNE to reduce dimension (optional)\n",
    " - Python’s Matplotlib (optional)\n",
    "\n",
    "_Note: The most important hyper parameters of skip-gram/CBOW are vector size and windows size_\n",
    "\n",
    "This assignment  will be broadly  split into **2 parts**.\n",
    "\n",
    "#### Part I\n",
    "##### Preparation:\n",
    "Download and extract the Google’s pretrained Word2Vec model (Google has  trained a large corpus of text containing billions of words,). To kick things off we will use this pre trained model to explore Word2Vec. \n",
    "(Download Link: https://docs.google.com/a/berkeley.edu/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download)\n",
    "Now load this pretrained model in Gensim and you should be good to get started with this assignment. \n",
    "\n",
    "\n",
    "\n",
    "Q1: Find the **cosine similarities** between the following word pairs/tuples:\n",
    "- (France, England)\n",
    "- (smaller, bigger)\n",
    "- (England, London)\n",
    "- (France, Rocket)\n",
    "- (big, bigger)\n",
    "\n",
    "Q2 : Write an expression to extract the vector representations   of the words  (France,  England, smaller, bigger, rocket, big). \n",
    "\n",
    "Q3: Repeat the exercise from Q1 by finding the **euclidean distances** between the word pairs.\n",
    "\n",
    "Q4: What is the relationship between the magnitude of individual vectors, the vectors themselves and the cosine distance for any pair of words. Use any tuple in Q1 as an example to support your answer. \n",
    "\n",
    "Q5: Time to dabble with the power of Word2Vec. Find the 2 closest words  for the following conditions:  \n",
    "- (King - Queen)\n",
    "- (bigger - big + small)\n",
    "- (man + programmer - woman)\n",
    "- (school + shooting - guns)\n",
    "- (Texas + Milwaukee – Wisconsin)\n",
    "\n",
    "Q6: Using the vectors for the following words, explore the semantic representation of these words through K-means clustering and explain your findings.\n",
    "\n",
    "Q7: What loss function does the skipgram model use and briefly describe what this function is minimizing .\n",
    "\n",
    "\n",
    "#### Part II:\n",
    "\n",
    "In part 1 we used the Word2Vec model on a pre trained corpus. In this part (in the next lab) you are going to train a Word2Vec model on your own dataset/corpus(text). Choose a text corpus (A good place to start will be the nltk corpus, the gutenburg project or the brown movie reviews) and tokenize the text (We will go through this in detail in the next Lab.) \n",
    "\n",
    "You can also choose the the dataset provided here.\n",
    "\n",
    "Q7. Based on your knowledge or understand of the text corpus you have chosen, form 3 hypotheses of analogies or relationships you expect will hold and give a reason why.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity_for_2_words(model, left, right):\n",
    "    M = np.array([model[left], model[right]])\n",
    "    M_sim = cosine_similarity(M)\n",
    "    return M_sim[0][1]\n",
    "\n",
    "def euc_similarity_for_2_words(model, left, right):\n",
    "    return np.linalg.norm(model[left]-model[right])\n",
    "\n",
    "def print_cos_similarity_for_word_pair(model, pair):\n",
    "    print(\"{}: {:.4f} \".format(pair, cos_similarity_for_2_words(model, pair[0], pair[1])))\n",
    "\n",
    "def print_euc_similarity_for_word_pair(model, pair):\n",
    "    print(\"{}: {:.4f} \".format(pair, euc_similarity_for_2_words(model, pair[0], pair[1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Find the cosine similarities between the following word pairs/tuples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (France, England)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('France', 'England'): 0.3980 \n"
     ]
    }
   ],
   "source": [
    "print_cos_similarity_for_word_pair(model, ('France', 'England'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (smaller, bigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('smaller', 'bigger'): 0.7302 \n"
     ]
    }
   ],
   "source": [
    "print_cos_similarity_for_word_pair(model, ('smaller', 'bigger'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (England, London)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('England', 'London'): 0.4399 \n"
     ]
    }
   ],
   "source": [
    "print_cos_similarity_for_word_pair(model, ('England', 'London'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (France, Rocket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('France', 'Rocket'): 0.0711 \n"
     ]
    }
   ],
   "source": [
    "print_cos_similarity_for_word_pair(model, ('France', 'Rocket'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (big, bigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('big', 'bigger'): 0.6842 \n"
     ]
    }
   ],
   "source": [
    "print_cos_similarity_for_word_pair(model, ('big', 'bigger'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 : Write an expression to extract the vector representations of the following words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### France "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.85839844e-02,  7.86132812e-02,  3.24218750e-01,  3.49121094e-02,\n",
       "        7.71484375e-02,  3.54003906e-02, -1.25976562e-01, -3.86718750e-01,\n",
       "       -1.31835938e-01,  2.91748047e-02, -1.44531250e-01, -1.42578125e-01,\n",
       "        1.79687500e-01, -2.75390625e-01, -1.65039062e-01,  9.32617188e-02,\n",
       "        1.17187500e-01,  1.82617188e-01,  6.10351562e-02,  1.14257812e-01,\n",
       "        1.82617188e-01, -1.16699219e-01, -3.24707031e-02, -7.56835938e-02,\n",
       "        9.64355469e-03,  8.59375000e-02, -2.85156250e-01, -2.55859375e-01,\n",
       "        3.01513672e-02,  2.16796875e-01, -1.00097656e-01,  2.85644531e-02,\n",
       "       -2.81250000e-01, -8.39843750e-02, -2.02636719e-02, -1.96289062e-01,\n",
       "       -4.78515625e-02,  7.12890625e-02, -1.42578125e-01, -1.13525391e-02,\n",
       "        1.16210938e-01,  7.22656250e-02,  1.47460938e-01,  1.50390625e-01,\n",
       "        1.40625000e-01,  2.47070312e-01, -1.69921875e-01,  7.76367188e-02,\n",
       "       -5.44433594e-02,  1.66992188e-01, -1.45507812e-01,  2.12402344e-02,\n",
       "       -7.51953125e-02,  4.58984375e-02, -2.55859375e-01,  1.49414062e-01,\n",
       "       -5.62500000e-01, -1.34765625e-01, -1.87500000e-01, -3.26538086e-03,\n",
       "        8.44726562e-02,  1.33789062e-01,  2.99072266e-02, -2.92968750e-01,\n",
       "       -1.56250000e-01,  2.50244141e-03, -1.10473633e-02, -7.08007812e-02,\n",
       "        1.90429688e-02,  1.56250000e-01, -4.95605469e-02,  2.08007812e-01,\n",
       "       -3.66210938e-02,  2.07031250e-01,  1.27929688e-01,  2.91748047e-02,\n",
       "       -4.88281250e-03, -6.31713867e-03,  6.49414062e-02,  1.66015625e-01,\n",
       "       -1.08032227e-02, -1.83593750e-01,  1.49414062e-01, -1.71875000e-01,\n",
       "        1.85546875e-01, -9.86328125e-02, -2.10937500e-01, -2.06298828e-02,\n",
       "        1.02050781e-01,  1.41601562e-01, -1.21093750e-01,  5.93261719e-02,\n",
       "       -2.89062500e-01,  7.47070312e-02,  3.11279297e-02, -2.21679688e-01,\n",
       "        1.94335938e-01, -4.62890625e-01, -6.78710938e-02, -1.91650391e-02,\n",
       "       -2.39257812e-01, -1.09863281e-01, -1.45507812e-01,  1.81640625e-01,\n",
       "        1.27929688e-01, -1.41601562e-01,  8.39843750e-02, -6.56127930e-03,\n",
       "        8.83789062e-02,  2.75878906e-02, -5.61523438e-03,  8.88671875e-02,\n",
       "        2.25585938e-01, -3.49121094e-02,  1.72851562e-01, -2.64892578e-02,\n",
       "        1.61743164e-03,  2.17773438e-01,  3.51562500e-01,  1.61132812e-01,\n",
       "       -8.20312500e-02, -1.36718750e-01,  2.20947266e-02, -2.96630859e-02,\n",
       "        2.77343750e-01, -1.67968750e-01, -3.08837891e-02, -1.59179688e-01,\n",
       "       -4.00390625e-01,  8.93554688e-02,  2.99072266e-02,  1.82617188e-01,\n",
       "       -3.06640625e-01, -1.74804688e-01, -9.09423828e-03, -4.51660156e-02,\n",
       "        4.34570312e-02, -1.25732422e-02,  1.57226562e-01, -7.66601562e-02,\n",
       "        2.36328125e-01, -1.10839844e-01,  2.11914062e-01, -5.29785156e-02,\n",
       "        1.22558594e-01,  9.39941406e-03,  1.25976562e-01,  4.85839844e-02,\n",
       "       -4.76074219e-02, -2.09960938e-01, -1.42578125e-01,  1.34277344e-02,\n",
       "       -3.35693359e-03,  8.39843750e-02,  1.03515625e-01, -3.14453125e-01,\n",
       "        8.48388672e-03,  1.78710938e-01,  9.47265625e-02,  1.61132812e-01,\n",
       "       -4.76074219e-02,  1.91406250e-01,  2.09960938e-01, -7.17773438e-02,\n",
       "        2.94921875e-01, -1.96289062e-01, -1.78710938e-01,  1.10351562e-01,\n",
       "       -1.77001953e-02,  9.66796875e-02, -1.20605469e-01,  1.40625000e-01,\n",
       "        6.34765625e-02, -1.93359375e-01, -7.27539062e-02, -6.64062500e-02,\n",
       "        4.46777344e-02, -1.25000000e-01,  1.26953125e-01, -1.02539062e-01,\n",
       "        1.30859375e-01, -2.37304688e-01, -3.10546875e-01, -2.39257812e-01,\n",
       "        4.23828125e-01, -3.55529785e-03,  1.85546875e-01,  2.38281250e-01,\n",
       "       -1.48437500e-01,  1.91406250e-01,  1.20117188e-01,  5.73730469e-02,\n",
       "        1.11816406e-01,  9.52148438e-03, -1.42822266e-02,  3.33984375e-01,\n",
       "        1.17675781e-01, -8.44726562e-02, -1.66992188e-01, -2.12890625e-01,\n",
       "        1.13281250e-01,  3.71093750e-02, -6.00585938e-02, -1.61132812e-01,\n",
       "        1.28906250e-01,  1.63085938e-01, -1.76757812e-01, -2.05078125e-01,\n",
       "        7.95898438e-02, -7.95898438e-02,  5.39550781e-02, -1.28906250e-01,\n",
       "       -8.64257812e-02,  3.75000000e-01, -1.26953125e-01, -1.11328125e-01,\n",
       "        1.81884766e-02, -1.55273438e-01,  1.50390625e-01, -4.27246094e-02,\n",
       "       -1.30859375e-01, -1.69921875e-01, -6.16455078e-03, -3.06396484e-02,\n",
       "       -1.56250000e-01,  7.37304688e-02,  2.59765625e-01,  8.49609375e-02,\n",
       "        3.71093750e-01,  9.22851562e-02, -9.81445312e-02, -3.39355469e-02,\n",
       "        2.01171875e-01,  7.12890625e-02, -5.49316406e-02,  3.61328125e-02,\n",
       "       -9.32617188e-02,  7.56835938e-02, -2.29492188e-01, -8.15429688e-02,\n",
       "        8.88671875e-02, -6.03027344e-02,  1.64062500e-01,  2.35351562e-01,\n",
       "        1.41601562e-01,  7.03125000e-02, -3.90625000e-01, -1.15722656e-01,\n",
       "       -2.25585938e-01, -1.12792969e-01,  8.59375000e-02,  7.81250000e-02,\n",
       "       -2.20947266e-02,  1.56250000e-01,  4.76074219e-02,  1.19140625e-01,\n",
       "        3.61328125e-02, -1.98242188e-01,  7.47070312e-02,  1.72851562e-01,\n",
       "        6.98242188e-02,  2.75390625e-01, -3.36914062e-02,  2.49023438e-01,\n",
       "        2.17773438e-01, -6.03027344e-02, -1.47460938e-01, -2.92968750e-01,\n",
       "       -1.18164062e-01,  1.43554688e-01, -1.95312500e-01,  7.56835938e-02,\n",
       "       -3.56445312e-02, -1.07910156e-01,  7.51953125e-02,  9.81445312e-02,\n",
       "       -6.59179688e-02,  2.65625000e-01, -1.48437500e-01,  4.93164062e-02,\n",
       "       -2.37304688e-01,  2.50244141e-02,  8.69750977e-04, -3.47900391e-03,\n",
       "       -8.25195312e-02,  2.43164062e-01, -2.49023438e-02,  1.49414062e-01,\n",
       "       -6.64062500e-02,  4.95605469e-02,  1.38671875e-01,  3.16406250e-01,\n",
       "       -6.58035278e-05,  1.62109375e-01, -1.54296875e-01, -7.71484375e-02,\n",
       "       -1.19628906e-01, -2.22167969e-02,  1.60156250e-01, -7.17773438e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['France']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.98242188e-01,  1.15234375e-01,  6.25000000e-02, -5.83496094e-02,\n",
       "        2.26562500e-01,  4.58984375e-02, -6.22558594e-02, -2.02148438e-01,\n",
       "        8.05664062e-02,  2.16064453e-02, -2.79541016e-02, -1.21093750e-01,\n",
       "        1.24511719e-01,  5.39550781e-02,  3.29589844e-02,  6.88476562e-02,\n",
       "       -5.12695312e-02,  1.83593750e-01,  1.32812500e-01, -7.22656250e-02,\n",
       "        1.06933594e-01,  2.50244141e-03, -1.37695312e-01,  1.75781250e-02,\n",
       "        1.06933594e-01,  1.08398438e-01, -2.34375000e-01,  8.05664062e-02,\n",
       "        3.73535156e-02,  2.61718750e-01,  7.42187500e-02,  9.21630859e-03,\n",
       "       -2.77343750e-01, -1.75781250e-01, -7.61718750e-02, -2.44140625e-02,\n",
       "       -1.26953125e-01, -9.37500000e-02,  5.41992188e-02,  3.08593750e-01,\n",
       "        2.16064453e-02,  1.19628906e-02,  1.66992188e-01, -4.39453125e-02,\n",
       "       -2.96630859e-02,  2.81982422e-02,  1.26953125e-01,  1.12304688e-01,\n",
       "        3.85742188e-02,  1.69921875e-01, -1.08886719e-01,  3.16406250e-01,\n",
       "        2.51953125e-01, -1.21093750e-01, -6.64062500e-02,  8.30078125e-03,\n",
       "       -3.35937500e-01,  9.37500000e-02, -2.55859375e-01, -1.15234375e-01,\n",
       "       -4.54101562e-02,  9.61914062e-02, -2.57812500e-01, -2.63671875e-01,\n",
       "       -8.69140625e-02,  6.17675781e-02,  8.83789062e-02,  1.04003906e-01,\n",
       "       -9.70458984e-03,  1.40625000e-01, -2.47802734e-02, -2.67578125e-01,\n",
       "       -9.52148438e-02,  4.46777344e-02,  3.36914062e-02, -7.51953125e-02,\n",
       "       -5.27343750e-02,  1.51367188e-01,  9.08203125e-02, -2.38281250e-01,\n",
       "        1.10473633e-02, -4.44335938e-02,  7.56835938e-02, -3.00781250e-01,\n",
       "        1.40625000e-01, -1.51367188e-01,  1.08398438e-01, -1.25000000e-01,\n",
       "        2.89062500e-01,  1.76429749e-04,  3.04687500e-01,  2.27050781e-02,\n",
       "       -3.71093750e-01, -9.33837891e-03, -2.67578125e-01,  1.83593750e-01,\n",
       "        1.88476562e-01, -3.43750000e-01, -7.17773438e-02, -2.68554688e-02,\n",
       "       -2.15530396e-04, -3.85742188e-02,  5.10253906e-02,  3.73046875e-01,\n",
       "        1.00585938e-01, -1.53320312e-01,  3.61328125e-02, -1.55639648e-02,\n",
       "        7.61718750e-02, -1.28906250e-01, -1.35742188e-01,  6.15234375e-02,\n",
       "       -6.83593750e-02,  6.15234375e-02,  2.15820312e-01, -2.73437500e-01,\n",
       "        1.56250000e-01,  8.15429688e-02,  5.00000000e-01,  1.75781250e-01,\n",
       "        2.73437500e-02, -2.89062500e-01,  2.16796875e-01, -4.54101562e-02,\n",
       "       -9.76562500e-02, -6.68945312e-02,  2.34375000e-02,  7.03125000e-02,\n",
       "       -4.16015625e-01,  9.76562500e-02, -1.11328125e-01, -1.44531250e-01,\n",
       "       -2.67578125e-01, -4.85839844e-02,  7.27539062e-02, -5.71289062e-02,\n",
       "       -6.64062500e-02,  3.04687500e-01, -4.95605469e-02, -3.17382812e-02,\n",
       "        7.71484375e-02,  9.61914062e-02,  1.57226562e-01, -1.17675781e-01,\n",
       "        2.19726562e-02,  1.05468750e-01,  5.06591797e-03, -1.42211914e-02,\n",
       "        1.81579590e-03,  3.78417969e-02, -1.06445312e-01, -1.97265625e-01,\n",
       "        1.94335938e-01,  8.59375000e-02,  4.66308594e-02, -1.90429688e-01,\n",
       "       -2.24609375e-01, -6.98242188e-02,  5.88378906e-02,  1.94335938e-01,\n",
       "       -3.36914062e-02,  3.46679688e-02,  4.06250000e-01, -6.15234375e-02,\n",
       "       -4.88281250e-02, -3.10546875e-01,  3.32031250e-02, -1.48437500e-01,\n",
       "       -3.69262695e-03,  1.11816406e-01, -5.39062500e-01,  1.18164062e-01,\n",
       "       -1.34765625e-01, -4.27246094e-02,  4.66308594e-02, -2.03125000e-01,\n",
       "       -1.61132812e-01, -2.14843750e-01, -4.27246094e-02, -1.09863281e-01,\n",
       "        1.39160156e-02, -2.53906250e-01,  1.08398438e-01, -2.81250000e-01,\n",
       "        2.99072266e-02,  1.29882812e-01,  1.64062500e-01, -3.51562500e-02,\n",
       "        3.67187500e-01,  1.25000000e-01, -2.31445312e-01, -4.00390625e-02,\n",
       "        1.48315430e-02, -1.88476562e-01,  7.47070312e-02,  2.94921875e-01,\n",
       "        5.41992188e-02, -5.52368164e-03, -3.69140625e-01, -2.13867188e-01,\n",
       "        1.15722656e-01,  9.71679688e-02,  1.64794922e-02, -4.22363281e-02,\n",
       "       -6.44531250e-02,  2.31445312e-01, -1.53320312e-01, -8.34960938e-02,\n",
       "        1.52343750e-01, -1.23535156e-01, -1.15722656e-01, -1.91406250e-01,\n",
       "        1.71875000e-01,  3.36914062e-02, -2.94921875e-01, -2.63671875e-01,\n",
       "        1.18164062e-01, -9.08203125e-02, -5.78613281e-02,  6.44531250e-02,\n",
       "       -1.63085938e-01, -8.10546875e-02,  4.24804688e-02, -2.01171875e-01,\n",
       "       -7.37304688e-02,  2.01171875e-01,  1.31225586e-02, -2.94921875e-01,\n",
       "       -9.86328125e-02, -2.47070312e-01, -1.53320312e-01,  6.68945312e-02,\n",
       "        2.08984375e-01,  5.68847656e-02, -3.03955078e-02,  1.51367188e-01,\n",
       "        8.59375000e-02,  6.68945312e-02, -1.51367188e-01, -2.33398438e-01,\n",
       "        2.25585938e-01, -2.04101562e-01,  3.04687500e-01,  4.51660156e-02,\n",
       "       -2.79296875e-01,  8.20312500e-02, -4.39453125e-02, -1.23046875e-01,\n",
       "        1.55029297e-02,  3.24218750e-01,  2.02148438e-01, -1.33056641e-02,\n",
       "        5.83496094e-02,  6.86645508e-03,  2.19726562e-01,  1.97265625e-01,\n",
       "       -1.47460938e-01, -1.22680664e-02,  1.19628906e-01,  3.44238281e-02,\n",
       "       -1.89208984e-02,  1.15722656e-01,  2.01171875e-01,  1.78710938e-01,\n",
       "        2.08984375e-01, -1.10351562e-01, -1.91406250e-01, -1.79687500e-01,\n",
       "        1.54296875e-01, -2.16796875e-01, -1.23535156e-01,  1.81640625e-01,\n",
       "        2.08984375e-01, -6.39648438e-02,  2.06054688e-01, -2.36328125e-01,\n",
       "        2.65625000e-01,  8.39843750e-02,  8.00781250e-02, -3.22265625e-01,\n",
       "       -4.73022461e-03,  1.89453125e-01,  2.83203125e-01,  5.02929688e-02,\n",
       "       -1.29882812e-01,  2.97851562e-02,  2.44140625e-01,  4.56542969e-02,\n",
       "        4.49218750e-02,  1.11328125e-01,  1.35742188e-01,  1.09375000e-01,\n",
       "       -1.21582031e-01,  8.54492188e-03, -1.71875000e-01,  8.69140625e-02,\n",
       "        7.03125000e-02,  3.28063965e-03,  6.93359375e-02,  5.61523438e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['England']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05004883,  0.03417969, -0.0703125 ,  0.17578125,  0.00689697,\n",
       "       -0.13183594,  0.03686523, -0.04638672, -0.01092529,  0.10058594,\n",
       "        0.03173828,  0.12011719,  0.06005859,  0.0859375 , -0.18652344,\n",
       "       -0.10888672, -0.20507812,  0.10107422, -0.22070312,  0.06103516,\n",
       "       -0.05200195,  0.0189209 , -0.05688477, -0.00646973, -0.20410156,\n",
       "        0.01623535, -0.24316406,  0.04077148,  0.16113281, -0.13769531,\n",
       "       -0.125     ,  0.11230469,  0.09326172, -0.09082031,  0.10009766,\n",
       "       -0.1796875 , -0.03930664,  0.2109375 ,  0.15625   ,  0.33007812,\n",
       "        0.21679688,  0.08984375,  0.11035156, -0.01141357, -0.06689453,\n",
       "        0.140625  , -0.0859375 ,  0.07470703,  0.02148438, -0.27539062,\n",
       "        0.10107422, -0.265625  , -0.27539062, -0.30078125, -0.05444336,\n",
       "        0.07128906,  0.01708984, -0.01446533,  0.14941406, -0.11914062,\n",
       "       -0.05004883,  0.07080078, -0.203125  , -0.11962891,  0.05078125,\n",
       "       -0.10205078, -0.07128906,  0.2734375 , -0.12109375,  0.28320312,\n",
       "        0.19238281, -0.10644531,  0.26953125,  0.13574219, -0.29882812,\n",
       "       -0.09228516, -0.04467773,  0.09667969, -0.02514648, -0.21484375,\n",
       "        0.00772095,  0.02954102, -0.08447266,  0.14648438, -0.20214844,\n",
       "       -0.20019531, -0.07373047,  0.21777344,  0.22460938,  0.11083984,\n",
       "        0.02294922, -0.07373047, -0.11279297,  0.12158203,  0.00610352,\n",
       "       -0.06396484,  0.10986328, -0.14941406, -0.16894531, -0.20117188,\n",
       "        0.09814453, -0.12060547,  0.19238281, -0.03710938,  0.04467773,\n",
       "        0.0859375 ,  0.1484375 ,  0.08886719,  0.02246094, -0.15332031,\n",
       "        0.01916504, -0.00531006,  0.01965332, -0.00262451,  0.12353516,\n",
       "        0.12402344,  0.04296875, -0.2265625 ,  0.30273438,  0.12011719,\n",
       "       -0.00817871,  0.00982666, -0.04589844,  0.05395508,  0.00213623,\n",
       "        0.05102539,  0.21875   , -0.10546875,  0.03149414, -0.01080322,\n",
       "        0.14355469,  0.02124023,  0.10546875,  0.29101562,  0.04394531,\n",
       "       -0.05786133, -0.04223633, -0.16894531, -0.10351562, -0.16308594,\n",
       "       -0.02331543, -0.14941406,  0.02453613, -0.23242188,  0.33203125,\n",
       "       -0.15136719, -0.20507812, -0.04638672,  0.05419922, -0.0378418 ,\n",
       "       -0.17871094, -0.05395508, -0.26953125,  0.12451172, -0.12597656,\n",
       "       -0.0378418 , -0.07714844, -0.00540161, -0.1875    ,  0.17675781,\n",
       "        0.06982422,  0.08642578, -0.14355469,  0.01055908,  0.24121094,\n",
       "        0.05957031,  0.15917969, -0.01275635,  0.04956055, -0.10253906,\n",
       "       -0.25976562, -0.26367188, -0.16503906, -0.13964844, -0.08496094,\n",
       "       -0.01501465,  0.05078125, -0.06689453, -0.08007812,  0.05883789,\n",
       "       -0.203125  , -0.18066406, -0.19140625,  0.0480957 , -0.05639648,\n",
       "       -0.07666016,  0.11962891,  0.09716797, -0.02758789, -0.08837891,\n",
       "        0.01452637, -0.25      , -0.234375  , -0.2265625 , -0.11279297,\n",
       "       -0.09130859,  0.06835938, -0.13671875,  0.20703125, -0.06445312,\n",
       "        0.29882812,  0.16699219,  0.08837891, -0.13671875, -0.01483154,\n",
       "       -0.10253906,  0.0222168 , -0.17285156, -0.20703125,  0.09619141,\n",
       "        0.04956055,  0.0859375 ,  0.00119781,  0.2734375 , -0.13183594,\n",
       "       -0.04516602,  0.19628906, -0.07568359, -0.01281738,  0.12304688,\n",
       "        0.15722656,  0.12011719, -0.07470703, -0.2109375 , -0.3515625 ,\n",
       "       -0.25976562, -0.09423828,  0.25      ,  0.1328125 ,  0.18261719,\n",
       "       -0.02294922,  0.078125  ,  0.15136719,  0.22070312,  0.07128906,\n",
       "       -0.14355469, -0.18652344, -0.41601562, -0.07958984,  0.07421875,\n",
       "        0.07080078, -0.05224609, -0.06201172, -0.14160156,  0.15527344,\n",
       "       -0.13964844,  0.16015625,  0.08154297,  0.0559082 , -0.01513672,\n",
       "        0.09863281, -0.0456543 ,  0.14648438, -0.07666016,  0.14257812,\n",
       "        0.11914062, -0.0402832 ,  0.14453125, -0.07714844,  0.20019531,\n",
       "        0.06787109, -0.015625  , -0.3046875 ,  0.13476562,  0.04711914,\n",
       "       -0.11767578, -0.24609375,  0.04858398, -0.16601562,  0.20703125,\n",
       "       -0.04370117,  0.05053711,  0.11376953, -0.05859375,  0.05615234,\n",
       "       -0.26367188, -0.10107422, -0.00683594,  0.11376953, -0.0703125 ,\n",
       "       -0.23535156,  0.09472656, -0.05932617, -0.18164062, -0.01696777,\n",
       "        0.09277344, -0.08544922, -0.00054932, -0.01251221,  0.05102539,\n",
       "       -0.18261719, -0.08056641, -0.14453125, -0.05786133, -0.10205078,\n",
       "        0.19140625, -0.00714111,  0.06201172,  0.0279541 , -0.0255127 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['smaller']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.54296875e-02, -9.52148438e-02, -6.22558594e-02,  1.62109375e-01,\n",
       "        1.98974609e-02, -1.74804688e-01,  1.16210938e-01, -7.42187500e-02,\n",
       "       -3.41796875e-02,  2.33398438e-01,  1.03027344e-01, -3.17382812e-02,\n",
       "        1.44531250e-01,  3.95507812e-02, -1.33789062e-01,  1.08337402e-03,\n",
       "        6.25000000e-02,  6.83593750e-02,  7.12890625e-02,  1.49414062e-01,\n",
       "       -1.55273438e-01,  7.71484375e-02,  1.06933594e-01,  1.15234375e-01,\n",
       "       -2.16796875e-01, -8.59375000e-02, -2.59765625e-01, -2.90527344e-02,\n",
       "        1.61132812e-01,  1.13281250e-01, -1.04003906e-01,  3.61328125e-01,\n",
       "       -7.89642334e-04, -1.38671875e-01,  1.02539062e-01, -1.95312500e-01,\n",
       "        5.12695312e-02,  2.50000000e-01,  3.94531250e-01,  4.16015625e-01,\n",
       "        2.08984375e-01, -5.10253906e-02,  1.37695312e-01, -4.46777344e-02,\n",
       "       -7.81250000e-02,  8.20312500e-02,  2.06298828e-02,  1.72851562e-01,\n",
       "        7.08007812e-02, -2.45117188e-01,  3.71093750e-02,  6.68334961e-03,\n",
       "       -2.63671875e-01, -3.63281250e-01, -9.64355469e-03, -1.23535156e-01,\n",
       "        9.91210938e-02, -1.10839844e-01,  3.20312500e-01, -3.24218750e-01,\n",
       "       -7.66601562e-02,  1.88476562e-01, -1.15722656e-01, -8.15429688e-02,\n",
       "        2.90527344e-02,  6.98242188e-02,  1.21459961e-02,  2.10937500e-01,\n",
       "       -1.07421875e-01,  2.51953125e-01,  2.04101562e-01, -8.05664062e-02,\n",
       "        3.61328125e-01,  1.92382812e-01, -3.69140625e-01, -9.86328125e-02,\n",
       "        2.05993652e-03,  2.71484375e-01,  2.08740234e-02, -2.83203125e-01,\n",
       "        4.67300415e-04, -2.01416016e-02, -6.98242188e-02,  1.42578125e-01,\n",
       "       -2.83203125e-01, -2.46093750e-01, -3.61328125e-02,  1.16210938e-01,\n",
       "        1.35742188e-01,  4.02832031e-02,  2.59765625e-01, -1.40625000e-01,\n",
       "       -2.67578125e-01, -1.36718750e-01, -6.12792969e-02, -1.73828125e-01,\n",
       "        2.14843750e-01,  1.00097656e-01, -6.15234375e-02, -1.27929688e-01,\n",
       "        2.75878906e-02, -4.00543213e-04,  3.22265625e-01, -1.88476562e-01,\n",
       "       -2.14843750e-02,  9.81445312e-02, -5.00488281e-02, -2.46582031e-02,\n",
       "       -3.07617188e-02, -1.65039062e-01,  5.17578125e-02, -1.18164062e-01,\n",
       "        5.51757812e-02,  1.47460938e-01,  1.15234375e-01,  1.50390625e-01,\n",
       "        6.73828125e-02, -8.59375000e-02,  1.10839844e-01,  1.11328125e-01,\n",
       "       -8.10546875e-02, -3.89099121e-03, -2.04101562e-01,  1.13281250e-01,\n",
       "       -7.95898438e-02,  8.54492188e-03,  1.58203125e-01, -3.36914062e-02,\n",
       "       -1.04492188e-01,  4.58984375e-02, -1.05590820e-02,  6.17675781e-02,\n",
       "       -1.11816406e-01,  1.97265625e-01,  1.63085938e-01, -2.04101562e-01,\n",
       "        9.08203125e-02, -5.32226562e-02, -9.42382812e-02, -2.58789062e-02,\n",
       "        1.78222656e-02, -1.44531250e-01,  9.81445312e-02, -2.41210938e-01,\n",
       "        1.81640625e-01, -3.08593750e-01, -1.07421875e-01, -5.78613281e-02,\n",
       "       -3.38745117e-03, -6.05468750e-02, -2.42187500e-01, -1.98364258e-03,\n",
       "       -2.65625000e-01,  4.19921875e-02, -2.07031250e-01,  3.95507812e-02,\n",
       "       -1.61132812e-01, -2.38281250e-01, -4.19921875e-02,  1.34765625e-01,\n",
       "        1.92382812e-01,  1.31835938e-01, -6.64062500e-02, -7.08007812e-02,\n",
       "        2.87109375e-01, -2.97851562e-02,  1.27929688e-01, -2.03125000e-01,\n",
       "        4.12597656e-02,  1.13769531e-01, -2.17773438e-01, -1.32812500e-01,\n",
       "        2.12402344e-02, -1.27929688e-01, -1.60156250e-01,  1.73828125e-01,\n",
       "       -1.14746094e-02, -5.20019531e-02,  9.22851562e-02,  4.61425781e-02,\n",
       "       -5.29785156e-02, -1.68945312e-01,  6.03027344e-02, -8.10546875e-02,\n",
       "       -1.66992188e-01, -2.42919922e-02, -1.70898438e-02,  1.16699219e-01,\n",
       "       -3.11279297e-02, -5.15136719e-02,  2.05078125e-01, -1.98242188e-01,\n",
       "        4.30297852e-03,  2.12402344e-02, -2.56347656e-02, -1.23535156e-01,\n",
       "       -5.37109375e-03, -2.74658203e-02,  5.46875000e-02, -8.83789062e-02,\n",
       "        1.89453125e-01,  1.67968750e-01,  1.74560547e-02, -2.25585938e-01,\n",
       "        1.46484375e-01, -4.37011719e-02,  1.22558594e-01, -2.11914062e-01,\n",
       "       -1.22558594e-01,  1.00097656e-01, -2.24609375e-02,  3.41796875e-01,\n",
       "        1.25976562e-01,  1.95312500e-01, -4.57763672e-03, -1.22070312e-01,\n",
       "        2.63671875e-01, -5.37109375e-02,  1.69677734e-02,  1.66015625e-01,\n",
       "       -2.73437500e-02,  1.36718750e-01, -2.21252441e-03, -1.78710938e-01,\n",
       "       -1.34765625e-01, -3.57421875e-01, -2.38281250e-01,  1.98242188e-01,\n",
       "        2.40478516e-02,  7.22656250e-02, -1.75781250e-01,  1.85546875e-01,\n",
       "        1.36718750e-01,  2.59765625e-01,  3.71093750e-02, -5.20019531e-02,\n",
       "       -1.35742188e-01, -5.66406250e-01, -1.85546875e-02, -3.54003906e-02,\n",
       "        1.08886719e-01,  2.72216797e-02, -2.61718750e-01, -4.44335938e-02,\n",
       "       -3.73535156e-02, -7.61718750e-02,  1.50390625e-01,  1.52587891e-02,\n",
       "        1.52343750e-01, -2.70996094e-02,  1.76757812e-01,  8.30078125e-02,\n",
       "        1.25976562e-01,  8.93554688e-02,  2.08984375e-01,  1.62109375e-01,\n",
       "       -1.45507812e-01,  8.69140625e-02, -2.53906250e-01,  1.33789062e-01,\n",
       "       -9.91210938e-02, -6.78710938e-02, -2.69531250e-01,  5.98144531e-02,\n",
       "        2.16064453e-02, -4.71191406e-02, -2.67578125e-01,  9.37500000e-02,\n",
       "       -1.81640625e-01,  1.70898438e-01, -8.78906250e-02,  6.49414062e-02,\n",
       "        2.36328125e-01, -6.22558594e-02,  1.27563477e-02, -1.06933594e-01,\n",
       "       -1.26953125e-01, -7.61718750e-02, -9.64355469e-03,  6.10351562e-02,\n",
       "       -1.50390625e-01,  6.15234375e-02, -1.87500000e-01, -1.44531250e-01,\n",
       "        4.83398438e-02,  2.63671875e-01, -1.25732422e-02, -9.91210938e-02,\n",
       "        1.27929688e-01, -1.83105469e-02, -1.08642578e-02, -2.64892578e-02,\n",
       "       -1.19628906e-01,  3.70025635e-04, -2.06054688e-01,  1.71875000e-01,\n",
       "       -1.33789062e-01, -1.11328125e-01, -1.04492188e-01, -1.97265625e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['bigger']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.19824219e-02,  2.71484375e-01, -2.89062500e-01, -1.54296875e-01,\n",
       "        1.68945312e-01, -3.65234375e-01,  3.33984375e-01, -4.39453125e-01,\n",
       "        1.75781250e-01,  2.48046875e-01,  2.79541016e-02, -1.39648438e-01,\n",
       "       -6.88476562e-02, -1.62109375e-01, -1.57226562e-01,  1.58203125e-01,\n",
       "       -3.59375000e-01,  1.26953125e-02,  1.85546875e-01, -3.08593750e-01,\n",
       "       -1.52343750e-01,  1.87500000e-01, -2.17773438e-01,  6.59179688e-03,\n",
       "       -1.59263611e-04, -7.37304688e-02,  4.95605469e-02, -5.32226562e-02,\n",
       "        2.81250000e-01, -2.21679688e-01, -3.59375000e-01, -4.43359375e-01,\n",
       "       -1.02050781e-01, -1.63085938e-01,  2.96020508e-03, -3.30078125e-01,\n",
       "        3.41796875e-02,  2.38281250e-01, -1.19140625e-01,  6.34765625e-02,\n",
       "       -1.53320312e-01, -9.47265625e-02,  3.37890625e-01,  2.28515625e-01,\n",
       "        1.25732422e-02, -1.08398438e-01,  1.21582031e-01, -5.24902344e-03,\n",
       "       -3.36914062e-02, -3.28125000e-01, -2.77343750e-01,  3.16406250e-01,\n",
       "        1.60156250e-01,  3.22265625e-02, -2.17773438e-01,  9.57031250e-02,\n",
       "        2.34375000e-01,  3.12500000e-02, -5.00488281e-02, -2.01171875e-01,\n",
       "       -2.23632812e-01, -9.82666016e-03, -9.22851562e-02, -6.20117188e-02,\n",
       "       -1.51367188e-01, -3.78906250e-01,  1.89453125e-01, -1.92382812e-01,\n",
       "       -2.65625000e-01,  3.12500000e-01, -8.83789062e-02, -4.76562500e-01,\n",
       "        2.14843750e-01, -1.08886719e-01, -4.37500000e-01,  1.82617188e-01,\n",
       "        2.59399414e-03, -1.43554688e-01, -3.73046875e-01,  6.25000000e-02,\n",
       "       -2.01416016e-02,  3.80859375e-02,  2.78320312e-02, -2.81982422e-02,\n",
       "        6.49414062e-02, -1.47460938e-01, -2.12097168e-03,  8.05664062e-02,\n",
       "        2.33398438e-01,  6.54296875e-02, -2.42919922e-02, -2.96875000e-01,\n",
       "       -2.01171875e-01, -1.83593750e-01,  1.54296875e-01,  1.87988281e-02,\n",
       "        2.04101562e-01,  1.52343750e-01, -2.69531250e-01,  4.02343750e-01,\n",
       "        2.61718750e-01, -3.73046875e-01,  1.67968750e-01, -1.18164062e-01,\n",
       "       -2.39257812e-01,  1.91406250e-01,  2.77343750e-01,  2.83203125e-01,\n",
       "        1.76757812e-01,  1.35498047e-02, -1.88476562e-01, -1.08398438e-01,\n",
       "       -1.51367188e-01,  1.04492188e-01, -2.12890625e-01, -2.23632812e-01,\n",
       "       -2.08984375e-01,  2.67578125e-01, -5.32226562e-02, -2.25585938e-01,\n",
       "       -3.95507812e-02,  1.12915039e-02,  9.27734375e-02, -1.31835938e-02,\n",
       "        1.71875000e-01, -3.86718750e-01,  1.55273438e-01, -9.81445312e-02,\n",
       "       -1.69921875e-01,  9.81445312e-02,  8.88671875e-02, -3.04687500e-01,\n",
       "       -5.51757812e-02, -1.01562500e-01,  2.27539062e-01,  1.13281250e-01,\n",
       "        4.76562500e-01, -2.10937500e-01,  1.22070312e-01, -2.10937500e-01,\n",
       "       -9.66796875e-02, -1.48437500e-01,  2.14843750e-01, -5.98144531e-02,\n",
       "       -1.66992188e-01, -3.53515625e-01, -1.39648438e-01, -3.47656250e-01,\n",
       "       -1.16699219e-01,  7.56835938e-02,  3.39843750e-01,  3.11279297e-02,\n",
       "        1.04003906e-01,  3.19824219e-02, -2.24609375e-01, -3.61328125e-01,\n",
       "       -4.83398438e-02,  1.96838379e-03,  1.94549561e-04, -1.74804688e-01,\n",
       "        2.91015625e-01,  9.03320312e-02, -1.48315430e-02, -1.06933594e-01,\n",
       "       -6.93359375e-02, -5.58593750e-01,  5.58593750e-01,  1.70898438e-01,\n",
       "        4.69970703e-03, -5.85937500e-02,  6.00585938e-02, -8.10546875e-02,\n",
       "       -4.27246094e-02, -2.68554688e-03,  1.05468750e-01, -1.24511719e-02,\n",
       "        1.47460938e-01, -3.12500000e-02, -3.18359375e-01,  8.25195312e-02,\n",
       "       -1.70898438e-01, -2.40234375e-01, -4.71191406e-02,  2.63671875e-02,\n",
       "        2.83203125e-01, -1.75781250e-01, -2.71484375e-01,  1.45507812e-01,\n",
       "       -7.71484375e-02, -1.24511719e-01, -2.28515625e-01,  1.74560547e-02,\n",
       "        2.12890625e-01,  1.82617188e-01, -2.91015625e-01,  2.42187500e-01,\n",
       "       -1.92382812e-01,  1.86523438e-01,  1.63085938e-01, -1.39648438e-01,\n",
       "        1.50390625e-01, -1.32812500e-01, -1.87500000e-01,  7.22656250e-02,\n",
       "       -1.54296875e-01, -2.08007812e-01, -3.66210938e-02, -5.15625000e-01,\n",
       "       -9.96093750e-02, -4.58984375e-02, -1.15722656e-01,  6.44531250e-02,\n",
       "        1.74804688e-01, -3.55529785e-03, -3.10546875e-01,  1.20605469e-01,\n",
       "       -4.73022461e-03,  3.43750000e-01,  1.30615234e-02, -2.02636719e-02,\n",
       "        4.31640625e-01, -2.42919922e-02,  3.92578125e-01, -1.93359375e-01,\n",
       "        1.80664062e-01,  1.32812500e-01,  1.77734375e-01,  5.51757812e-02,\n",
       "        1.40625000e-01, -1.31835938e-01,  1.79687500e-01, -1.56250000e-02,\n",
       "       -2.28271484e-02,  3.24218750e-01,  1.77734375e-01, -2.98828125e-01,\n",
       "       -6.21093750e-01,  2.73437500e-02,  2.65625000e-01, -5.85937500e-01,\n",
       "        9.42382812e-02, -2.25585938e-01, -9.61914062e-02, -5.31250000e-01,\n",
       "       -2.10937500e-01, -2.67578125e-01,  1.57226562e-01, -5.03906250e-01,\n",
       "        2.99072266e-02, -7.72094727e-03, -2.31445312e-01, -3.04687500e-01,\n",
       "        2.14843750e-01,  1.45507812e-01,  3.32031250e-01,  9.81445312e-02,\n",
       "       -1.62353516e-02, -2.24609375e-01,  1.99218750e-01, -4.80468750e-01,\n",
       "        1.36718750e-01,  6.98852539e-03, -1.40380859e-02,  2.91015625e-01,\n",
       "        3.18359375e-01, -1.13769531e-01, -1.54296875e-01, -8.64257812e-02,\n",
       "       -1.00097656e-02,  4.02832031e-02,  5.68847656e-02,  8.30078125e-03,\n",
       "        6.15234375e-02, -1.28906250e-01, -1.47460938e-01, -5.29785156e-02,\n",
       "       -1.84570312e-01,  6.68945312e-02,  5.78613281e-02,  2.24609375e-01,\n",
       "        9.42382812e-02,  5.15625000e-01,  1.70898438e-02, -8.69140625e-02,\n",
       "        1.80664062e-01, -2.92968750e-01,  6.88476562e-02,  4.04296875e-01,\n",
       "        2.47070312e-01,  1.25976562e-01,  1.01562500e-01,  2.47070312e-01,\n",
       "       -3.08593750e-01, -7.56835938e-02,  5.24902344e-02, -5.51757812e-02,\n",
       "       -6.83593750e-03,  1.56250000e-01, -3.90625000e-01, -1.59179688e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['rocket']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11132812,  0.10595703, -0.07373047,  0.18847656,  0.07666016,\n",
       "       -0.3828125 , -0.0625    , -0.07470703,  0.05957031,  0.22167969,\n",
       "        0.20507812, -0.09228516,  0.05395508,  0.01379395, -0.16992188,\n",
       "        0.05493164,  0.09619141,  0.06103516, -0.14160156,  0.03173828,\n",
       "       -0.08642578,  0.12011719,  0.06445312,  0.22070312,  0.06835938,\n",
       "        0.04956055, -0.22460938, -0.06298828,  0.09179688, -0.00531006,\n",
       "       -0.11425781,  0.20605469,  0.31054688, -0.0625    , -0.02026367,\n",
       "       -0.13476562, -0.02697754,  0.2734375 ,  0.27929688,  0.21386719,\n",
       "        0.25195312, -0.13964844,  0.19824219, -0.07421875,  0.09228516,\n",
       "        0.125     ,  0.0612793 , -0.02990723,  0.0072937 , -0.05615234,\n",
       "       -0.08447266,  0.1796875 , -0.17578125, -0.11328125, -0.17578125,\n",
       "       -0.1171875 ,  0.09082031, -0.07177734,  0.30273438, -0.2734375 ,\n",
       "       -0.07128906,  0.33007812, -0.13574219, -0.0390625 ,  0.01397705,\n",
       "       -0.02526855,  0.05981445,  0.14550781, -0.11035156,  0.12988281,\n",
       "        0.12695312, -0.04980469,  0.16992188,  0.18261719, -0.23144531,\n",
       "        0.07910156, -0.06738281,  0.34960938, -0.07324219, -0.03442383,\n",
       "        0.01507568, -0.05957031, -0.07373047, -0.03857422, -0.06542969,\n",
       "       -0.2578125 , -0.05151367,  0.08154297,  0.08740234,  0.01318359,\n",
       "        0.06640625,  0.06152344, -0.20800781, -0.10253906, -0.12158203,\n",
       "       -0.06152344,  0.1484375 ,  0.06005859, -0.19140625, -0.05761719,\n",
       "       -0.03149414,  0.12255859,  0.19628906, -0.24902344,  0.03735352,\n",
       "        0.08056641,  0.13183594, -0.01977539,  0.19238281, -0.109375  ,\n",
       "       -0.02172852, -0.09912109, -0.00793457, -0.01251221,  0.23144531,\n",
       "       -0.01080322,  0.00234985,  0.11279297, -0.00485229,  0.01373291,\n",
       "       -0.02441406,  0.01123047, -0.22460938,  0.14160156, -0.00744629,\n",
       "       -0.01385498,  0.20996094,  0.05712891,  0.00366211, -0.07617188,\n",
       "       -0.05859375, -0.05957031,  0.00613403,  0.13183594,  0.04003906,\n",
       "       -0.11376953,  0.046875  ,  0.04199219, -0.0088501 , -0.01672363,\n",
       "        0.03173828, -0.08398438,  0.16308594, -0.17285156, -0.00866699,\n",
       "       -0.04443359,  0.10595703,  0.0145874 ,  0.04492188, -0.07568359,\n",
       "       -0.06689453, -0.0050354 , -0.10986328,  0.11572266, -0.10107422,\n",
       "        0.16210938, -0.0144043 , -0.140625  , -0.09326172, -0.02502441,\n",
       "       -0.13867188,  0.04760742, -0.01452637, -0.10986328,  0.31640625,\n",
       "       -0.11914062,  0.08203125, -0.22949219,  0.09472656, -0.0324707 ,\n",
       "       -0.18847656, -0.10986328,  0.22949219, -0.22363281, -0.07080078,\n",
       "        0.05053711,  0.20507812,  0.03979492, -0.03881836,  0.11962891,\n",
       "        0.01818848, -0.2421875 ,  0.10791016, -0.02307129, -0.06982422,\n",
       "        0.05737305, -0.15527344,  0.01989746, -0.01153564, -0.04833984,\n",
       "        0.16210938,  0.01916504,  0.06079102,  0.01177979, -0.07714844,\n",
       "       -0.02380371, -0.10693359, -0.06542969, -0.20214844, -0.07373047,\n",
       "        0.25390625,  0.12353516,  0.09130859, -0.1484375 ,  0.03662109,\n",
       "        0.02294922,  0.09375   , -0.13183594,  0.03857422,  0.13378906,\n",
       "       -0.13183594,  0.30859375, -0.0255127 ,  0.07763672, -0.09033203,\n",
       "        0.00350952,  0.32617188, -0.08203125, -0.01831055,  0.01879883,\n",
       "       -0.0390625 ,  0.17578125, -0.05273438, -0.22070312, -0.10302734,\n",
       "        0.0390625 , -0.00315857, -0.06176758, -0.01397705, -0.04003906,\n",
       "       -0.17773438,  0.20507812,  0.05004883,  0.10595703, -0.00170898,\n",
       "       -0.04223633, -0.02478027, -0.28320312, -0.08789062, -0.08349609,\n",
       "        0.12792969,  0.04931641, -0.16503906, -0.07275391,  0.00405884,\n",
       "       -0.23828125,  0.16992188, -0.06689453,  0.07958984, -0.14160156,\n",
       "        0.01287842,  0.1640625 ,  0.24511719,  0.09570312,  0.18261719,\n",
       "       -0.00179291, -0.00714111,  0.20507812, -0.40429688, -0.01940918,\n",
       "       -0.07373047, -0.16113281, -0.20800781,  0.00860596, -0.08642578,\n",
       "        0.0189209 , -0.08642578,  0.03930664,  0.03564453,  0.14550781,\n",
       "       -0.01806641,  0.14746094,  0.10888672, -0.06982422, -0.02209473,\n",
       "       -0.01361084, -0.03039551, -0.13574219, -0.0625    ,  0.04003906,\n",
       "       -0.13964844,  0.02026367, -0.12597656,  0.0703125 , -0.03015137,\n",
       "        0.11376953,  0.00337219, -0.06347656, -0.1484375 , -0.11572266,\n",
       "       -0.17675781, -0.08984375, -0.09667969, -0.11669922, -0.09082031,\n",
       "       -0.02490234, -0.00509644, -0.07226562,  0.03735352, -0.15625   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['big']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Find the euclidian similarities between the following word pairs/tuples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (France, England)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('France', 'England'): 3.0151 \n"
     ]
    }
   ],
   "source": [
    "print_euc_similarity_for_word_pair(model, ('France', 'England'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (smaller, bigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('smaller', 'bigger'): 1.8619 \n"
     ]
    }
   ],
   "source": [
    "print_euc_similarity_for_word_pair(model, ('smaller', 'bigger'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (England, London)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('England', 'London'): 2.8753 \n"
     ]
    }
   ],
   "source": [
    "print_euc_similarity_for_word_pair(model, ('England', 'London'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (France, Rocket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('France', 'Rocket'): 3.8921 \n"
     ]
    }
   ],
   "source": [
    "print_euc_similarity_for_word_pair(model, ('France', 'Rocket'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (big, bigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('big', 'bigger'): 1.9586 \n"
     ]
    }
   ],
   "source": [
    "print_euc_similarity_for_word_pair(model, ('big', 'bigger'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: What is the relationship between the magnitude of individual vectors, the vectors themselves and the cosine distance for any pair of words. Use any tuple in Q1 as an example to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Measures\n",
    "- Cosine similarity is expressed $\\in$ $[0,1]$ where $1$ means that two elements are 100% similiar.\n",
    "- Euclidian similarity is expressed $\\in$ $[0, \\infty[$ where $0$ means that two elements are 100% similiar.\n",
    "\n",
    "#### Cosine Similarity\n",
    "\n",
    "The cosine similarity of two vectors is defined as follows: \n",
    "$cos(A,B) = \\frac{\\sum_{i=1}^{n}{A_i B_i}}{\\sqrt{\\sum_{i=1}^{n}{A_i^2}} \\sqrt{\\sum_{i=1}^{n}{B_i^2}} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Vector for one word :: 300\n",
      "(Min, Max, Mean) of the vector's values for the word \"England\" :: (-0.5390625,0.5,-0.0010115846525877714)\n"
     ]
    }
   ],
   "source": [
    "print('Length of the Vector for one word ::', len(model['England']))\n",
    "print('(Min, Max, Mean) of the vector\\'s values for the word \"England\" ::', '({},{},{})'.format(np.min(model['England']), np.max(model['England']), np.mean(model['England'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Time to dabble with the power of Word2Vec. Find the 2 closest words for the following conditions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (King - Queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bon_Jovi_Canillas', 0.36876529455184937), ('Lomax', 0.36203983426094055)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['King'], negative=['Queen'], topn=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (bigger - big + small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('larger', 0.7402472496032715), ('smaller', 0.732999324798584)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['bigger', 'small'], negative=['big'], topn=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (man + programmer - woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('programer', 0.5371963977813721), ('programmers', 0.5310998558998108)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['man', 'programmer'], negative=['woman'], topn=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (school + shooting - guns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elementary', 0.5435296297073364), ('eighth_grade', 0.47330963611602783)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['school','shooting'], negative=['guns'], topn=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Texas + Milwaukee – Wisconsin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Houston', 0.7767744660377502), ('Fort_Worth', 0.7270511388778687)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['Texas', 'Milwaukee'], negative=['Wisconsin'], topn=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6: Using the vectors for the following words, explore the semantic representation of these words through K-means clustering and explain your findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Note: I do not know based on the description which words are \"the following words\". \n",
    "# Also, there is no response from the discussion on bcourses, as of now. \n",
    "#(https://bcourses.berkeley.edu/courses/1468844/discussion_topics/5334270)\n",
    "\n",
    "# I therefore chose to cluster the first 25.000 words of the vocabulary of the model.\n",
    "\n",
    "\n",
    "# all words in the vocabulary of the model\n",
    "words = list(model.vocab) \n",
    "words_25k = words[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001129</td>\n",
       "      <td>-0.000896</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>-0.001404</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000420</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000824</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>-0.001564</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>-0.108887</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>0.020752</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088867</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>0.061279</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.047607</td>\n",
       "      <td>0.014465</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.011780</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.063965</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>-0.082520</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.007935</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>0.024170</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.015747</td>\n",
       "      <td>-0.028320</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.050293</td>\n",
       "      <td>-0.110352</td>\n",
       "      <td>0.031738</td>\n",
       "      <td>-0.014221</td>\n",
       "      <td>-0.089844</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.020630</td>\n",
       "      <td>-0.019409</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>-0.148438</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007050</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.106934</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.001129 -0.000896  0.000319  0.001534  0.001106 -0.001404 -0.000031   \n",
       "1  0.070312  0.086914  0.087891  0.062500  0.069336 -0.108887 -0.081543   \n",
       "2 -0.011780 -0.047363  0.044678  0.063477 -0.018188 -0.063965 -0.001312   \n",
       "3 -0.015747 -0.028320  0.083496  0.050293 -0.110352  0.031738 -0.014221   \n",
       "4  0.007050 -0.073242  0.171875  0.022583 -0.132812  0.198242  0.112793   \n",
       "\n",
       "          7         8         9  ...        291       292       293       294  \\\n",
       "0 -0.000420 -0.000576  0.001076  ...   0.001373 -0.000061 -0.000824  0.001328   \n",
       "1 -0.154297  0.020752  0.131836  ...  -0.088867 -0.080566  0.064941  0.061279   \n",
       "2 -0.072266  0.064453  0.086426  ...   0.003723 -0.082520  0.081543  0.007935   \n",
       "3 -0.089844  0.117676  0.118164  ...  -0.015625 -0.033447 -0.020630 -0.019409   \n",
       "4 -0.107910  0.071777  0.020874  ...  -0.036377 -0.093750  0.182617  0.027100   \n",
       "\n",
       "        295       296       297       298       299  word  \n",
       "0  0.001160  0.000568 -0.001564 -0.000123 -0.000086  </s>  \n",
       "1 -0.047363 -0.058838 -0.047607  0.014465 -0.062500    in  \n",
       "2  0.000477  0.018433  0.071289 -0.034912  0.024170   for  \n",
       "3  0.063965  0.020142  0.006866  0.061035 -0.148438  that  \n",
       "4  0.127930 -0.024780  0.011230  0.164062  0.106934    is  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = words_25k\n",
    "\n",
    "[model[w] for w in ws]\n",
    "\n",
    "df = pd.DataFrame( [model[w] for w in ws])\n",
    "df['word'] = ws\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] ::  0.5811315260411222\n",
      "[3] ::  0.04608681286669453\n",
      "[4] ::  0.002498790204143555\n",
      "[5] ::  -0.006186132170751177\n",
      "[6] ::  -0.030888140054599608\n",
      "[7] ::  -0.02849355112494855\n",
      "[8] ::  -0.02240108134000604\n",
      "[20] ::  -0.04449654988168943\n",
      "[40] ::  -0.029422972407840912\n",
      "[80] ::  -0.022621608065209214\n"
     ]
    }
   ],
   "source": [
    "X = df.set_index('word')\n",
    "results = []\n",
    "for i in [2,3,4,5,6,7,8,20,40,80]:\n",
    "    kmeans = KMeans(n_clusters=i, random_state=1).fit(X)\n",
    "    score = silhouette_score(X, kmeans.labels_)\n",
    "\n",
    "    print('[' + str(i) + '] :: ', score)\n",
    "    results.append((i, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The rapid decline of the silhoutte coefficient suggests that the best clustering is found for **k=2** for the selected words.\n",
    "However, given that I selected the first 25.000 words, without respect to their semantic meaning, the optimal cluster sizes on a sub- or superset will most probably differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7: What loss function does the skipgram model use and briefly describe what this function is minimizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "\n",
    "The TensorFlow documentation provides a detailed writeup of how Word2Vec and the Skipgram model work.\n",
    "\n",
    "- **Step 1:** Given a text, build the tuples of $(context, target)$ where $target$ refers to a word in the text and $context$ to the neighboring words in the text, e.g. $([the, brown], quick))$\n",
    "- **Step 2:** Expand the $(context, target)$ tuples, as to each pairwise combinations of $(input, output)$ tuples. E.g. $([the, brown], quick))$ becomes $(quick, the), (quick, brown), (brown, quick), (brown, fox), ...$.\n",
    "- **Step 3:** The model is trained using the $(input, output)$ tuples using **stochastic gradient descent** one tuple at a time. (also explains long training times)\n",
    "    - For each $(input, output)$ a number of **constrastive** (noisy) examples is drawn and the loss is is calculated using the following formula:\n",
    "\n",
    "$J^{(t)}_\\text{NEG} = \\log Q_\\theta(D=1 | \\text{the, quick}) +\n",
    "  \\log(Q_\\theta(D=0 | \\text{sheep, quick}))$\n",
    "  \n",
    "where $(the, quick)$ represents the training sample and $(sheep, quick)$ the contrastive sample with $sheep$ being the word drawn randomly.\n",
    "\n",
    "The goal is to then update the *embedding parameter* $\\theta$ to improve the objective function, by deriving the gradient of the loss with respect to $\\theta$: $\\frac{\\partial}{\\partial \\theta} J_\\text{NEG}$ and then taking a small step into the direction of the gradient.\n",
    "  \n",
    "\n",
    "**Src:** https://www.tensorflow.org/tutorials/word2vec#the_skip-gram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part II:\n",
    "In the next lab you are going to train a Word2Vec model on your own dataset/corpus(text). To prepare do the following...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a text corpus (A good place to start will be the nltk corpus, the gutenburg project or the brown movie reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** I decided to select [Grimms' Fairy Tales](https://www.gutenberg.org/ebooks/2591) from Project Gutenberg for the next lab and analyse it using the NLTK corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text (We will go through this in detail in the next Lab.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import string\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e2815454da7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Note: Code adapted from gensim_tutorial.ipynb from the current lab.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msent_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m### Download and load  \"The Importance of Being Earnest A Trivial Comedy for Serious People\" by Oscar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# Note: Code adapted from gensim_tutorial.ipynb from the current lab.\n",
    "\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "### Download and load  \"The Importance of Being Earnest A Trivial Comedy for Serious People\" by Oscar \n",
    "## Wilde from Project Gutenberg : https://www.gutenberg.org\n",
    "\n",
    "\n",
    "## URL of Grimms's Fairy Tales\n",
    "url = \"https://www.gutenberg.org/files/2591/2591-0.txt\" ## Your raw text file location \n",
    "resp = urlopen(url)\n",
    "raw = resp.read().decode('utf8')\n",
    "firstlook = tokenize.sent_tokenize(raw)\n",
    "\n",
    "pattern = r'''(?x)  # set flag to allow verbose regexps\n",
    "(?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "|\\w+(?:[-']\\w+)*    # words with optional internal hyphens\n",
    "|\\$?\\d+(?:\\.\\d+)?   # currency, e.g. $12.80 \n",
    "|\\.\\.\\.             # elipses\n",
    "|[.,;\"'?()-_`]      # these are separate tokens\n",
    "'''\n",
    "#print(nltk.regexp_tokenize(raw,pattern))\n",
    "tokenized_raw = ' '.join( nltk.regexp_tokenize(raw,pattern))\n",
    "tokenized_raw= tokenize.sent_tokenize(tokenized_raw)\n",
    "\n",
    "nopunct=[]\n",
    "for sent in tokenized_raw:\n",
    "    a=[w for w in sent.split() if w not in string.punctuation]\n",
    "    nopunct.append(' '.join(a))\n",
    "#create a set of stopwords\n",
    "tok_corp= [nltk.word_tokenize(sent) for sent in nopunct]\n",
    "\n",
    "### creating a list of unique words \n",
    "\n",
    "combined_list=[\" \".join(w) for w in tok_corp]\n",
    "unique_list=[]\n",
    "for sent in combined_list:\n",
    "    unique_list.append([w for w in sent.split()])\n",
    "unique_list=list(set([item for sublist in unique_list for item in sublist]))\n",
    "\n",
    "unique_words=unique_list\n",
    "\n",
    "### Its just one single command\n",
    "model = gensim.models.Word2Vec(tok_corp, min_count=1, size = 16, window=7)\n",
    "\n",
    "## Extracting the respective vectors corresponding to the words\n",
    "\n",
    "vector_list=[] ## n by d matrix containing words and their respective vectors\n",
    "for word in unique_words:\n",
    "    vector_list.append(model[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Based on your knowledge or understand of the text corpus you have chosen, form 3 hypotheses of analogies or relationships you expect will hold and give a reason why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** *I created the hyptotheses below based on my knowledge of the stories within the Brother Grimm's Fary Tales. I did not inspect the text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H1: The words 'wolf' and 'evil' will have a high similarity.\n",
    "\n",
    "**Reason:** The character of the wolf is commonly associated as the evil persona in fary tales. Examples are *Red Riding Hood*, *Three Little Pigs* and probably there are a whole bunch more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2: 'King' + 'Daughter' = 'Princess'\n",
    "\n",
    "**Reason:** My assumption here is that the above relationship is present in enough fary tales that it can be 'uncovered' by building a corpus out of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H3: 'Hansel' + 'sister' = 'Gretel'\n",
    "\n",
    "**Reason:** The last hypotheses is specific to one of the many fary tales — *Hansel and Gretel*. The are siblings, so I assume that the above relationship can be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
